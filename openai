open ai:

from langchain_openai import OpenAIEmbeddings
from langchain_experimental.text_splitter import SemanticChunker
from langchain.text_splitter import TokenTextSplitter
from langchain.docstore.document import Document
import math, os, tiktoken

def split_documents(docs, chunk_size=500, chunk_overlap=100):
    """
    Semantic chunking using OpenAI embeddings + tokenizer + metadata tagging.
    """
    # 1️⃣ Create OpenAI embedding model
    embed_model = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )

    # 2️⃣ Semantic chunking
    semantic_chunker = SemanticChunker(
        embed_model,
        breakpoint_threshold_type="percentile",
        breakpoint_threshold_amount=0.9,
        number_of_chunks=10
    )
    semantic_chunks = semantic_chunker.split_documents([d for d in docs])

    # 3️⃣ Token-based splitter
    tokenizer = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

    # 4️⃣ Accurate token counter for OpenAI models
    encoding = tiktoken.encoding_for_model("gpt-4o")

    processed_chunks = []
    for i, chunk in enumerate(semantic_chunks):
        text = chunk.page_content.strip()
        token_count = len(encoding.encode(text))

        total_lines = text.count("\n") + 1
        mid_line = math.ceil(total_lines / 2)

        metadata = {
            **chunk.metadata,
            "chunk_id": f"chunk_{i}",
            "token_count": token_count,
            "source_id": chunk.metadata.get("source", f"doc_{i}"),
            "start_line_est": max(1, mid_line - 1),
            "end_line_est": mid_line + 1,
        }

        processed_chunks.append(Document(page_content=text, metadata=metadata))

    return processed_chunks
==========================================================================================================================================

return ChatOpenAI(
        model=model_name,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_output_tokens,
        top_p=top_p,
        frequency_penalty=frequency_penalty,
        presence_penalty=presence_penalty,
    )
